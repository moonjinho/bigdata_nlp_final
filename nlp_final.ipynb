{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPViUxwOmoIlaQyXAsnl0t1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puthvbFN8rOv"
      },
      "source": [
        "필요한 라이브러리 import 및 데이터 가져오기\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "w2eios8fjDe_"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKzOf2w6lRbB"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "%matplotlib inline\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import re\r\n",
        "import sys\r\n",
        "from konlpy.tag import Okt\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "from nltk.corpus import stopwords \r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "\r\n",
        "train_data = pd.read_table('ratings_train.txt')\r\n",
        "test_data = pd.read_table('ratings_test.txt')"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZpkos6ELSZn"
      },
      "source": [
        "데이터 진행 과정 확인을 위한 Progress Bar Util"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHIRT1fqLJdg"
      },
      "source": [
        "def printProgress (iteration, total, prefix = '', suffix = '', decimals = 1, barLength = 100):\r\n",
        "    formatStr = \"{0:.\" + str(decimals) + \"f}\"\r\n",
        "    percent = formatStr.format(100 * (iteration / float(total)))\r\n",
        "    filledLength = int(round(barLength * iteration / float(total)))\r\n",
        "    bar = '#' * filledLength + '-' * (barLength - filledLength)\r\n",
        "    sys.stdout.write('\\r%s |%s| %s%s %s' % (prefix, bar, percent, '%', suffix)),\r\n",
        "    if iteration == total:\r\n",
        "        sys.stdout.write('\\n')\r\n",
        "    sys.stdout.flush()"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9FjlqvS9avC"
      },
      "source": [
        "훈련용 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iE5wOPk49eEj",
        "outputId": "b04b7263-19a2-4e62-c0e5-c10d3e5ca1e3"
      },
      "source": [
        "train_data.drop_duplicates(subset=['document'], inplace=True)                         # Sentence 열에서 중복인 내용이 있다면 중복 제거\r\n",
        "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\r\n",
        "train_data['document'].replace('', np.nan, inplace=True)                              # 공백 Nan으로 대체\r\n",
        "train_data = train_data.dropna(how = 'any')                                           # Nan 데이터 제거\r\n",
        "\r\n",
        "total_length = len(train_data)\r\n",
        "\r\n",
        "with open(r'kor_stopwords.txt',\"r\") as f:\r\n",
        "    stopwords = f.readlines()\r\n",
        "\r\n",
        "okt = Okt()\r\n",
        "\r\n",
        "stopwords = [x.strip() for x in stopwords] \r\n",
        "\r\n",
        "number = 0\r\n",
        "X_train = []\r\n",
        "for sentence in train_data['document']:\r\n",
        "    number += 1\r\n",
        "    printProgress(number, total_length, 'Progress:', 'Complete', 1, 100)\r\n",
        "    temp_X = []\r\n",
        "    temp_X = okt.morphs(sentence, stem=True)                    # 토큰화\r\n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # stopword 제거\r\n",
        "    X_train.append(temp_X)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress: |####################################################################################################| 100.0% Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7C1O85jA3DA"
      },
      "source": [
        "테스트용 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bdwCiR-oAzr4",
        "outputId": "87b3e2be-3320-4851-b13c-28ecdf77dd79"
      },
      "source": [
        "test_data.drop_duplicates(subset = ['document'], inplace=True)                      # Sentence 열에서 중복인 내용이 있다면 중복 제거\r\n",
        "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\r\n",
        "test_data['document'].replace('', np.nan, inplace=True)                             # 공백 Nan으로 대체\r\n",
        "test_data = test_data.dropna(how='any')                                             # Nan 데이터 제거\r\n",
        "\r\n",
        "total_length = len(test_data)\r\n",
        "\r\n",
        "with open(r'kor_stopwords.txt',\"r\") as f:\r\n",
        "    stopwords = f.readlines()\r\n",
        "\r\n",
        "okt = Okt()\r\n",
        "\r\n",
        "stopwords = [x.strip() for x in stopwords] \r\n",
        "\r\n",
        "number = 0\r\n",
        "X_test = []\r\n",
        "for sentence in test_data['document']:\r\n",
        "    number += 1\r\n",
        "    printProgress(number, total_length, 'Progress:', 'Complete', 1, 100)\r\n",
        "    temp_X = []\r\n",
        "    temp_X = okt.morphs(sentence, stem=True)                    # 토큰화\r\n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\r\n",
        "    X_test.append(temp_X)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress: |####################################################################################################| 100.0% Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLFGLRTPVzY9"
      },
      "source": [
        "정수 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "75WwOz7jV4O1"
      },
      "source": [
        "tokenizer = Tokenizer()\r\n",
        "tokenizer.fit_on_texts(X_train)\r\n",
        "\r\n",
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWPjNLPnWQ8D"
      },
      "source": [
        "단어 분포 확인 (3회 미만의 단어)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "T4NPy3duWVeE"
      },
      "source": [
        "threshold = 3\r\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\r\n",
        "rare_cnt = 0                          # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\r\n",
        "total_freq = 0                        # 훈련 데이터의 전체 단어 빈도수 총 합\r\n",
        "rare_freq = 0                         # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\r\n",
        "\r\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\r\n",
        "for key, value in tokenizer.word_counts.items():\r\n",
        "    total_freq = total_freq + value\r\n",
        "\r\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\r\n",
        "    if(value < threshold):\r\n",
        "        rare_cnt = rare_cnt + 1\r\n",
        "        rare_freq = rare_freq + value\r\n",
        "\r\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\r\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\r\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\r\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOyzajuvWlST"
      },
      "source": [
        "빈도수가 적은 단어들 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "yd67laYVWbbv"
      },
      "source": [
        "# 전체 단어들 중 빈도수 2이하인 단어 제거.\r\n",
        "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 + 2\r\n",
        "vocab_size = total_cnt - rare_cnt + 2\r\n",
        "print('단어 집합의 크기 :',vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhOYoWSOXL9c"
      },
      "source": [
        "위에서 설정한 특정 단어 집합의 크기(빈도수 3이상만)로 토크나이징"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwgq2OHeXDGf"
      },
      "source": [
        "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \r\n",
        "tokenizer.fit_on_texts(X_train)\r\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\r\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcbgwlQ6ONLD"
      },
      "source": [
        "label 값들 저장 후, 빈 샘플들 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "fK_ax2X9azFx"
      },
      "source": [
        "y_train = np.array(train_data['label'])\r\n",
        "y_test = np.array(test_data['label'])\r\n",
        "\r\n",
        "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\r\n",
        "\r\n",
        "X_train = np.delete(X_train, drop_train, axis=0)\r\n",
        "y_train = np.delete(y_train, drop_train, axis=0)\r\n",
        "\r\n",
        "print(len(X_train))\r\n",
        "print(len(y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iJRQNNQO0Aw"
      },
      "source": [
        "서로 다른 길이의 샘플들의 길이를 동일하게 작업 (패딩)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "vaV36l4LeZna"
      },
      "source": [
        "print('리뷰의 최대 길이 :',max(len(l) for l in X_train))\r\n",
        "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\r\n",
        "plt.hist([len(s) for s in X_train], bins=50)\r\n",
        "plt.xlabel('length of samples')\r\n",
        "plt.ylabel('number of samples')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr2JdaqgO9Eg"
      },
      "source": [
        "max_len 이하인 샘플의 비율이 몇 %인지 확인하는 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBOvdC-sekej"
      },
      "source": [
        "def below_threshold_len(max_len, nested_list):\r\n",
        "  cnt = 0\r\n",
        "  for s in nested_list:\r\n",
        "    if(len(s) <= max_len):\r\n",
        "        cnt = cnt + 1\r\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax0LJ4g3PMHY"
      },
      "source": [
        "max_len 이하 샘플의 비율 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxQQZjuYeqKZ"
      },
      "source": [
        "max_len = 30\r\n",
        "below_threshold_len(max_len, X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8Z8gPTbPSdO"
      },
      "source": [
        "모든 샘플의 길이를 일정 값(max_len)으로 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs-ePeDWe0JK"
      },
      "source": [
        "X_train = pad_sequences(X_train, maxlen = max_len)\r\n",
        "X_test = pad_sequences(X_test, maxlen = max_len)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7V_mTj6PmrS"
      },
      "source": [
        "LSTM으로 리뷰 감성 분류 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "8T6fy-V2e5OV"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(vocab_size, 100))\r\n",
        "model.add(LSTM(128))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\r\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\r\n",
        "\r\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\r\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IInnjPZ7Ptmj"
      },
      "source": [
        "가장 좋은 모델을 기준으로 테스트 데이터 정확도 측정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "3QLCqzwuye0f"
      },
      "source": [
        "loaded_model = load_model('best_model.h5')\r\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tedJmp1cP53V"
      },
      "source": [
        "새로운 문장이 들어오면 긍정/부정인지 예측하는 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDdMFkvTyyPN"
      },
      "source": [
        "def sentiment_predict(new_sentence):\r\n",
        "    with open(r'kor_stopwords.txt',\"r\") as f:\r\n",
        "        stopwords = f.readlines()\r\n",
        "\r\n",
        "    okt = Okt()\r\n",
        "\r\n",
        "    new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\r\n",
        "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\r\n",
        "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\r\n",
        "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\r\n",
        "    score = float(loaded_model.predict(pad_new)) # 예측\r\n",
        "\r\n",
        "    if (score > 0.5):\r\n",
        "        # print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\r\n",
        "        return 1\r\n",
        "    else:\r\n",
        "        # print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))\r\n",
        "        return 0"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-S0HcSpP_6U"
      },
      "source": [
        "Kaggle Sample 데이터를 통한 결과 분류 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN2sYwpH1I3i"
      },
      "source": [
        "predict_data = pd.read_csv('ko_data.csv', encoding='CP949')\r\n",
        "total_length = len(predict_data)\r\n",
        "\r\n",
        "cols = ['Id', 'Predicted']\r\n",
        "lst = []\r\n",
        "for index, row in predict_data.iterrows():\r\n",
        "    printProgress(row['Id'], total_length, 'Progress:', 'Complete', 1, 100)\r\n",
        "    lst.append([row['Id'], sentiment_predict(row['Sentence'])])\r\n",
        "\r\n",
        "result_df = pd.DataFrame(lst, columns=cols)\r\n",
        "print(result_df)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ8qh06nQETu"
      },
      "source": [
        "분석된 결과를 CSV 파일로 export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtI4Y_wa9ZdU"
      },
      "source": [
        "result_df.to_csv('sample.csv', sep=',', index=False)"
      ],
      "execution_count": 86,
      "outputs": []
    }
  ]
}